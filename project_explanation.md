# Ø´Ø±Ø­ ÙƒØ§Ù…Ù„ Ù„ÙƒÙˆØ¯ Ù…Ø´Ø±ÙˆØ¹ Research Paper Recommendation ğŸ“šğŸ’»

## 1ï¸âƒ£ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª (Importing Libraries)

```python
import numpy as np
import pandas as pd 
import os
import matplotlib.pyplot as plt
from ast import literal_eval
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
import pickle
```

**Ø§Ù„Ø´Ø±Ø­:**
- **numpy**: Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø­Ø³Ø§Ø¨ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù…ØµÙÙˆÙØ§Øª
- **pandas**: Ù„Ù‚Ø±Ø§Ø¡Ø© ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ©
- **matplotlib**: Ù„Ø±Ø³Ù… Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©
- **literal_eval**: Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ù„ÙƒØ§Ø¦Ù†Ø§Øª Python (Ù…Ø«Ù„ ØªØ­ÙˆÙŠÙ„ "[1,2,3]" Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ©)
- **train_test_split**: Ù„ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
- **tensorflow/keras**: Ù„Ù„Ù€ Deep Learning
- **TfidfVectorizer**: Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù…
- **pickle**: Ù„Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø©

---

## 2ï¸âƒ£ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Reading Data)

```python
df = pd.read_csv("arxiv_data_210930-054931.csv")
df.head()
```

**Ø§Ù„Ø´Ø±Ø­:**
- Ø¨ÙŠÙ‚Ø±Ø£ Ù…Ù„Ù CSV ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ÙˆØ±Ø§Ù‚ Ø§Ù„Ø¨Ø­Ø«ÙŠØ© Ù…Ù† ArXiv
- `df.head()` ÙŠØ¹Ø±Ø¶ Ø£ÙˆÙ„ 5 ØµÙÙˆÙ Ù„Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

---

## 3ï¸âƒ£ ØªÙ†Ø¸ÙŠÙ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Data Cleaning)

### Ø£) Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
```python
df['terms']                    # Ø¹Ø±Ø¶ Ø¹Ù…ÙˆØ¯ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª
df['terms'].unique()           # Ø¹Ø±Ø¶ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„ÙØ±ÙŠØ¯Ø©
df['terms'].value_counts()[:10] # Ø¹Ø±Ø¶ Ø£ÙƒØ«Ø± 10 ØªØµÙ†ÙŠÙØ§Øª ØªÙƒØ±Ø§Ø±Ø§Ù‹
```

### Ø¨) Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
```python
df.duplicated().sum()          # Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…ÙƒØ±Ø±Ø©
df.drop_duplicates(inplace=True) # Ø­Ø°Ù Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…ÙƒØ±Ø±Ø©
df.shape                       # Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù
```

### Ø¬) ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª
```python
df['terms'] = df['terms'].apply(literal_eval)  # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ©
```

**Ù…Ø«Ø§Ù„:**
```
Ù‚Ø¨Ù„: "['cs.AI', 'cs.LG']"      # Ù†Øµ
Ø¨Ø¹Ø¯: ['cs.AI', 'cs.LG']        # Ù‚Ø§Ø¦Ù…Ø© Python Ø­Ù‚ÙŠÙ‚ÙŠØ©
```

### Ø¯) Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ù†Ø§Ø¯Ø±Ø©
```python
term_counts = df['terms'].value_counts()       # Ø¹Ø¯ ØªÙƒØ±Ø§Ø± ÙƒÙ„ ØªØµÙ†ÙŠÙ
common_terms = term_counts[term_counts > 1].index  # Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ù…ØªÙƒØ±Ø±Ø© Ø£ÙƒØªØ± Ù…Ù† Ù…Ø±Ø©
df = df[df['terms'].isin(common_terms)]        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙÙ‚Ø·
```

**Ù„ÙŠÙ‡ØŸ** Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ù„ÙŠ ØªØ¸Ù‡Ø± Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¨Ø³ Ù…Ø´ Ù…ÙÙŠØ¯Ø© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬

---

## 4ï¸âƒ£ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Data Splitting)

```python
# ØªÙ‚Ø³ÙŠÙ… Ø±Ø¦ÙŠØ³ÙŠ: 90% ØªØ¯Ø±ÙŠØ¨ØŒ 10% Ø§Ø®ØªØ¨Ø§Ø±
train_df, test_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df['terms'])

# ØªÙ‚Ø³ÙŠÙ… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù„Ù†ØµÙŠÙ†
val_df = test_df.sample(frac=0.5, random_state=42)    # 50% Ù„Ù„ØªØ­Ù‚Ù‚
test_df = test_df.drop(val_df.index)                  # 50% Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ

print(f"Train size : {len(train_df)}")
print(f"Test size : {len(test_df)}")
print(f"Validation size : {len(val_df)}")
```

**Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:**
- **Training (90%)**: Ù„Ù„ØªØ¹Ù„Ù…
- **Validation (5%)**: Ù„Ø¶Ø¨Ø· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
- **Test (5%)**: Ù„Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ

**stratify=df['terms']**: ÙŠØ¶Ù…Ù† Ø£Ù† Ù†Ø³Ø¨ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ù…ØªØ´Ø§Ø¨Ù‡Ø© ÙÙŠ ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø©

---

## 5ï¸âƒ£ ØªØ­Ø¶ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª (Target Preparation)

```python
from sklearn.preprocessing import MultiLabelBinarizer

# Ø¥Ù†Ø´Ø§Ø¡ ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø­ÙˆÙ„
mlb = MultiLabelBinarizer()
mlb.fit(train_df['terms'])

# Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª (Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª)
vocab = mlb.classes_
print(f"Number of classes: {len(vocab)}")  # Ø¹Ø¯Ø¯ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„ÙØ±ÙŠØ¯Ø©

# ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø¥Ù„Ù‰ ØªØ´ÙÙŠØ± binary
train_labels = mlb.transform(train_df['terms'])
val_labels = mlb.transform(val_df['terms'])
test_labels = mlb.transform(test_df['terms'])
```

**Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ´ÙÙŠØ±:**
```python
# Ø§Ù„Ù…Ø¯Ø®Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ
sample = ['cs.AI', 'cs.LG']

# Ø¨Ø¹Ø¯ Ø§Ù„ØªØ´ÙÙŠØ± (Ù„Ùˆ Ø¹Ù†Ø¯Ù†Ø§ 5 ØªØµÙ†ÙŠÙØ§Øª Ù…Ø«Ù„Ø§Ù‹)
binarized = [1, 0, 1, 0, 0]  # 1 ÙŠØ¹Ù†ÙŠ Ø§Ù„ØªØµÙ†ÙŠÙ Ù…ÙˆØ¬ÙˆØ¯ØŒ 0 ÙŠØ¹Ù†ÙŠ Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯
```

### Ø¯Ø§Ù„Ø© Ø¹ÙƒØ³ Ø§Ù„ØªØ´ÙÙŠØ±
```python
def invert_multi_hot(encoded_labels):
    return [vocab[i] for i, val in enumerate(encoded_labels) if val == 1]
```

---

## 6ï¸âƒ£ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ (Text Vectorization)

```python
# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
vocabulary = set()
train_df['abstracts'].str.lower().str.split().apply(vocabulary.update)
vocab_size = len(vocabulary)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø­ÙˆÙ„ TF-IDF
tfidf_vectorizer = TfidfVectorizer(
    max_features=min(100000, vocab_size),  # Ø£Ù‚ØµÙ‰ Ø¹Ø¯Ø¯ Ù…ÙØ±Ø¯Ø§Øª
    ngram_range=(1, 2),                    # ÙƒÙ„Ù…Ø§Øª Ù…ÙØ±Ø¯Ø© + Ø£Ø²ÙˆØ§Ø¬ ÙƒÙ„Ù…Ø§Øª
    stop_words='english',                  # Ø¥Ø²Ø§Ù„Ø© ÙƒÙ„Ù…Ø§Øª Ù…Ø«Ù„ (the, a, an)
    min_df=5                               # Ø§Ù„ÙƒÙ„Ù…Ø© ØªØ¸Ù‡Ø± ÙÙŠ 5 ÙˆØ«Ø§Ø¦Ù‚ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„
)

# ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø­ÙˆÙ„ ÙˆØªØ·Ø¨ÙŠÙ‚Ù‡
tfidf_vectorizer.fit(train_df['abstracts'])
train_features = tfidf_vectorizer.transform(train_df['abstracts'])
val_features = tfidf_vectorizer.transform(val_df['abstracts'])
test_features = tfidf_vectorizer.transform(test_df['abstracts'])
```

**TF-IDF Ø´Ø±Ø­ Ù…Ø¨Ø³Ø·:**
- **TF** (Term Frequency): ÙƒØ§Ù… Ù…Ø±Ø© Ø§Ù„ÙƒÙ„Ù…Ø© Ø¸Ù‡Ø±Øª ÙÙŠ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©
- **IDF** (Inverse Document Frequency): Ù‚Ø¯ Ø¥ÙŠÙ‡ Ø§Ù„ÙƒÙ„Ù…Ø© Ù†Ø§Ø¯Ø±Ø© ÙÙŠ ÙƒÙ„ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
- **Ø§Ù„Ù†ØªÙŠØ¬Ø©**: Ø±Ù‚Ù… ÙŠÙˆØ¶Ø­ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ÙƒÙ„Ù…Ø© Ù„Ù„ÙˆØ«ÙŠÙ‚Ø© Ø¯ÙŠ

**Ù…Ø«Ø§Ù„:**
ÙƒÙ„Ù…Ø© "neural" ÙÙŠ ÙˆØ±Ù‚Ø© Ø¹Ù† Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©:
- Ù„Ùˆ Ø¸Ù‡Ø±Øª 5 Ù…Ø±Ø§Øª ÙÙŠ Ø§Ù„ÙˆØ±Ù‚Ø© (TF Ø¹Ø§Ù„ÙŠ)
- ÙˆÙ…Ø´ Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ ÙƒØªÙŠØ± ÙˆØ±Ù‚ ØªØ§Ù†ÙŠØ© (IDF Ø¹Ø§Ù„ÙŠ)
- Ø§Ù„Ù†ØªÙŠØ¬Ø©: Ø±Ù‚Ù… Ø¹Ø§Ù„ÙŠ = ÙƒÙ„Ù…Ø© Ù…Ù‡Ù…Ø© Ù„Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ±Ù‚Ø©

---

## 7ï¸âƒ£ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØµÙŠØºØ© TensorFlow

```python
import scipy.sparse as sp
from tensorflow.keras.utils import Sequence

class SparseDataGenerator(Sequence):
    """
    Ù…ÙˆÙ„Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙŠØ­ÙˆÙ„ Ø§Ù„Ù…ØµÙÙˆÙØ§Øª Ø§Ù„Ù…ØªÙ†Ø§Ø«Ø±Ø© Ø¥Ù„Ù‰ ÙƒØ«ÙŠÙØ© ÙÙŠ Ø¯ÙØ¹Ø§Øª ØµØºÙŠØ±Ø©
    Ù„ØªØ¬Ù†Ø¨ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    """
    def __init__(self, X_sparse, y, batch_size):
        self.X_sparse = X_sparse
        self.y = y
        self.batch_size = batch_size
        
    def __len__(self):
        return int(np.ceil(self.X_sparse.shape[0] / self.batch_size))
    
    def __getitem__(self, idx):
        # Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø¯ÙØ¹Ø©
        start_idx = idx * self.batch_size
        end_idx = min((idx + 1) * self.batch_size, self.X_sparse.shape[0])
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØªØ­ÙˆÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¯ÙØ¹Ø©
        X_batch = self.X_sparse[start_idx:end_idx].toarray()
        y_batch = self.y[start_idx:end_idx]
        
        return X_batch, y_batch
```

**Ù„ÙŠÙ‡ Ø§Ù„Ù…ÙˆÙ„Ø¯ Ø¯Ù‡ØŸ**
- Ø¨ÙŠØ§Ù†Ø§Øª TF-IDF ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
- ØªØ­Ù…ÙŠÙ„Ù‡Ø§ ÙƒÙ„Ù‡Ø§ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…Ù…ÙƒÙ† ÙŠØ®Ù„ÙŠ Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ± ÙŠØ¹Ù„Ù‚
- Ø§Ù„Ù…ÙˆÙ„Ø¯ ÙŠØ¯ÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¯ÙØ¹Ø§Øª ØµØºÙŠØ±Ø© ÙƒÙ„ Ù…Ø±Ø©

```python
# Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙˆÙ„Ø¯Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
batch_size = 128
train_generator = SparseDataGenerator(train_features, train_labels, batch_size)
val_generator = SparseDataGenerator(val_features, val_labels, batch_size)
test_generator = SparseDataGenerator(test_features, test_labels, batch_size)
```

---

## 8ï¸âƒ£ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Model Architecture)

```python
# Ø­Ø³Ø§Ø¨ Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ø¯Ø®Ù„ ÙˆØ§Ù„Ø®Ø±Ø¬
input_dim = train_features.shape[1]  # Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ù…Ù† TF-IDF
output_dim = len(vocab)              # Ø¹Ø¯Ø¯ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª

model = keras.Sequential([
    # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¯Ø®Ù„
    layers.Input(shape=(input_dim,)),
    
    # Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…Ø®ÙÙŠØ© Ø§Ù„Ø£ÙˆÙ„Ù‰
    layers.Dense(512),                    # 512 Ù†ÙŠÙˆØ±ÙˆÙ†
    layers.BatchNormalization(),          # ØªØ·Ø¨ÙŠØ¹ Ù„Ù„ØªØ³Ø±ÙŠØ¹ ÙˆØ§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±
    layers.Activation("relu"),            # Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙØ¹ÙŠÙ„
    layers.Dropout(0.5),                  # Ù…Ù†Ø¹ Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ø£Ø¹Ù…Ù‰
    
    # Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…Ø®ÙÙŠØ© Ø§Ù„Ø«Ø§Ù†ÙŠØ©
    layers.Dense(256),                    # 256 Ù†ÙŠÙˆØ±ÙˆÙ† (Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©)
    layers.BatchNormalization(),
    layers.Activation("relu"),
    layers.Dropout(0.5),
    
    # Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…Ø®ÙÙŠØ© Ø§Ù„Ø«Ø§Ù„Ø«Ø©
    layers.Dense(128),                    # 128 Ù†ÙŠÙˆØ±ÙˆÙ†
    layers.BatchNormalization(),
    layers.Activation("relu"),
    layers.Dropout(0.5),
    
    # Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…Ø®ÙÙŠØ© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø©
    layers.Dense(64),                     # 64 Ù†ÙŠÙˆØ±ÙˆÙ†
    layers.BatchNormalization(),
    layers.Activation("relu"),
    layers.Dropout(0.5),
    
    # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø®Ø±Ø¬
    layers.Dense(output_dim, activation='sigmoid')  # Sigmoid Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯
])
```

**Ø´Ø±Ø­ Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:**

### Dense Layers (Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ÙƒØ«ÙŠÙØ©)
- ÙƒÙ„ Ù†ÙŠÙˆØ±ÙˆÙ† Ù…ØªØµÙ„ Ø¨ÙƒÙ„ Ù†ÙŠÙˆØ±ÙˆÙ†Ø§Øª Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
- Ø§Ù„Ø£Ø±Ù‚Ø§Ù… (512, 256, 128, 64) ØªÙ‚Ù„ ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹ = ØªØ±ÙƒÙŠØ² Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª

### BatchNormalization
- Ø¨ÙŠØ·Ø¨Ø¹ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¹Ø´Ø§Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙŠÙƒÙˆÙ† Ø£Ø³Ø±Ø¹ ÙˆØ£ÙƒØ«Ø± Ø§Ø³ØªÙ‚Ø±Ø§Ø±Ø§Ù‹
- Ø²ÙŠ Ù…Ø§ ØªÙ‚ÙˆÙ„ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ "Ø®Ù„ÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù†ØªØ¸Ù…Ø©"

### ReLU Activation
```python
# Ø¯Ø§Ù„Ø© ReLU Ø¨Ø³ÙŠØ·Ø©
def relu(x):
    return max(0, x)  # Ù„Ùˆ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø³Ø§Ù„Ø¨Ø©ØŒ Ø®Ù„ÙŠÙ‡Ø§ ØµÙØ±
```

### Dropout
- Ø¨ÙŠØ´ÙŠÙ„ 50% Ù…Ù† Ø§Ù„Ø§ØªØµØ§Ù„Ø§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
- ÙŠÙ…Ù†Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù†Ù‡ ÙŠØ­ÙØ¸ Ø¨Ø¯Ù„ Ù…Ø§ ÙŠØªØ¹Ù„Ù…

### Sigmoid ÙÙŠ Ø§Ù„Ø®Ø±Ø¬
- Ø¨ÙŠØ·Ù„Ø¹ Ø£Ø±Ù‚Ø§Ù… Ø¨ÙŠÙ† 0 Ùˆ 1
- ÙƒÙ„ Ø±Ù‚Ù… ÙŠÙ…Ø«Ù„ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø¥Ù† Ø§Ù„ØªØµÙ†ÙŠÙ Ø¯Ù‡ Ù…ÙˆØ¬ÙˆØ¯

---

## 9ï¸âƒ£ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Model Training)

```python
# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨
model.compile(
    loss="binary_crossentropy",     # Ø¯Ø§Ù„Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯
    optimizer='adam',               # Ù…Ø­Ø³Ù† Ø³Ø±ÙŠØ¹ ÙˆÙØ¹Ø§Ù„
    metrics=['binary_accuracy']     # Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡
)

# Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø¨ÙƒØ±
early_stopping = EarlyStopping(
    patience=5,                     # Ù„Ùˆ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù…Ø§ ØªØ­Ø³Ù†Ø´ Ù„Ù€ 5 epochs
    restore_best_weights=True       # Ø§Ø±Ø¬Ø¹ Ù„Ø£Ø­Ø³Ù† ÙˆØ²Ù†
)

# Ø§Ù„ØªØ¯Ø±ÙŠØ¨
history = model.fit(
    train_generator,                # Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    validation_data=val_generator,  # Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ­Ù‚Ù‚
    epochs=20,                      # Ø¹Ø¯Ø¯ Ø§Ù„Ø¯ÙˆØ±Ø§Øª
    callbacks=[early_stopping]     # Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø¨ÙƒØ±
)
```

**Ø´Ø±Ø­ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…:**

### Binary Crossentropy
- Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯
- Ø¨ØªØ­Ø³Ø¨ Ø§Ù„Ø®Ø·Ø£ Ø¨ÙŠÙ† Ø§Ù„ØªÙˆÙ‚Ø¹ ÙˆØ§Ù„ÙˆØ§Ù‚Ø¹ Ù„ÙƒÙ„ ØªØµÙ†ÙŠÙ

### Adam Optimizer
- Ù…Ø­Ø³Ù† Ø°ÙƒÙŠ ÙŠØ¹Ø¯Ù„ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
- Ø£Ø³Ø±Ø¹ ÙˆØ£ÙƒØ«Ø± ÙØ¹Ø§Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ø­Ø³Ù†Ø§Øª Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ©

### Early Stopping
- Ù„Ùˆ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¹Ù„Ù‰ validation ØªÙˆÙ‚Ù Ø¹Ù† Ø§Ù„ØªØ­Ø³Ù† Ù„Ù€ 5 epochs
- ÙŠÙˆÙ‚Ù Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆÙŠØ±Ø¬Ø¹ Ù„Ø£Ø­Ø³Ù† ÙˆØ²Ù†
- ÙŠÙ…Ù†Ø¹ overfitting (Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ø£Ø¹Ù…Ù‰)

---

## ğŸ”Ÿ Ø±Ø³Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (Metrics Plotting)

```python
def plot_result(item):
    plt.plot(history.history[item], label=item)
    plt.plot(history.history["val_" + item], label="val_" + item)
    plt.xlabel("Epochs")
    plt.ylabel(item)
    plt.title("Train and Validation {} Over Epochs".format(item), fontsize=14)
    plt.legend()
    plt.grid()
    plt.show()

plot_result("loss")              # Ø±Ø³Ù… Ø§Ù„Ø®Ø³Ø§Ø±Ø©
plot_result("binary_accuracy")   # Ø±Ø³Ù… Ø§Ù„Ø¯Ù‚Ø©
```

**Ø§Ù„Ù‡Ø¯Ù:**
- Ù…ØªØ§Ø¨Ø¹Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
- Ø§Ù„ØªØ£ÙƒØ¯ Ø¥Ù†Ù‡ Ù…Ø´ Ø¨ÙŠØ­ÙØ¸ (Ù„Ùˆ Ø®Ø· validation Ø¨ÙŠØ±ØªÙØ¹ ÙˆØ§Ù„Ù€ training Ø¨ÙŠÙ†Ø²Ù„ = overfitting)

---

## 1ï¸âƒ£1ï¸âƒ£ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Model Evaluation)

```python
# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
train_loss, binary_acc_train = model.evaluate(train_generator)
val_loss, binary_acc_val = model.evaluate(val_generator)
test_loss, binary_acc_test = model.evaluate(test_generator)

print(f"Binary accuracy on the train set: {round(binary_acc_train * 100, 2)}%.")
print(f"Binary accuracy on the validation set: {round(binary_acc_val * 100, 2)}%.")
print(f"Binary accuracy on the test set: {round(binary_acc_test * 100, 2)}%.")
```

**Binary Accuracy Ø¥ÙŠÙ‡ØŸ**
- Ø¨ÙŠØ­Ø³Ø¨ ÙƒØ§Ù… ØªØµÙ†ÙŠÙ ØµØ­ Ù…Ù† Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª
- Ù…Ø«Ù„Ø§Ù‹: Ù„Ùˆ ÙˆØ±Ù‚Ø© ÙÙŠÙ‡Ø§ 3 ØªØµÙ†ÙŠÙØ§Øª ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ø±Ù 2 Ù…Ù†Ù‡Ù… = 66.7%

---

## 1ï¸âƒ£2ï¸âƒ£ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Save Model)

```python
# Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨
model.save("subject_area_model.keras")

# Ø­ÙØ¸ Ù…Ø­ÙˆÙ„ Ø§Ù„Ù†ØµÙˆØµ
with open('tfidf_vectorizer.pkl', 'wb') as f:
    pickle.dump(tfidf_vectorizer, f)

# Ø­ÙØ¸ Ù…Ø­ÙˆÙ„ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª
with open('label_binarizer.pkl', 'wb') as f:
    pickle.dump(mlb, f)
```

**Ù„ÙŠÙ‡ Ø¨Ù†Ø­ÙØ¸ 3 Ø­Ø§Ø¬Ø§ØªØŸ**
1. **Ø§Ù„Ù†Ù…ÙˆØ°Ø¬**: Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…Ø¯Ø±Ø¨
2. **TF-IDF**: Ø¹Ø´Ø§Ù† Ù†Ù‚Ø¯Ø± Ù†Ø­ÙˆÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù„Ø£Ø±Ù‚Ø§Ù…
3. **Label Binarizer**: Ø¹Ø´Ø§Ù† Ù†Ø±Ø¬Ø¹ Ù…Ù† Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ù„Ù„ØªØµÙ†ÙŠÙØ§Øª

---

## 1ï¸âƒ£3ï¸âƒ£ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­ÙÙˆØ¸ (Test Saved Model)

```python
from keras.models import load_model

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©
loaded_model = load_model("subject_area_model.keras")

with open('tfidf_vectorizer.pkl', 'rb') as f:
    loaded_vectorizer = pickle.load(f)

with open('label_binarizer.pkl', 'rb') as f:
    loaded_mlb = pickle.load(f)

loaded_vocab = loaded_mlb.classes_
```

### Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ†Ø¨Ø¤
```python
def predict_subject_areas(abstract_text, model, vectorizer, mlb, threshold=0.5):
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù…
    abstract_vector = vectorizer.transform([abstract_text])
    abstract_vector_dense = abstract_vector.toarray()
    
    # Ø§Ù„ØªÙ†Ø¨Ø¤
    predictions = model.predict(abstract_vector_dense)
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø¥Ù„Ù‰ Ù‚Ø±Ø§Ø±Ø§Øª (Ø£ÙƒØ¨Ø± Ù…Ù† 0.5 = Ù…ÙˆØ¬ÙˆØ¯)
    binary_predictions = (predictions[0] > threshold).astype(int)
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙØ§Øª
    predicted_subjects = [mlb.classes_[i] for i, val in enumerate(binary_predictions) if val == 1]
    
    return predicted_subjects
```

**ÙƒÙŠÙ ØªØ´ØªØºÙ„ØŸ**
1. ØªØ§Ø®Ø¯ Ø§Ù„Ù†Øµ
2. ØªØ­ÙˆÙ„Ù‡ Ù„Ø£Ø±Ù‚Ø§Ù… Ø¨Ù†ÙØ³ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
3. ØªØ¯Ø®Ù„Ù‡ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
4. Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ·Ù„Ø¹ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª (0 Ø¥Ù„Ù‰ 1)
5. Ø£ÙŠ Ø§Ø­ØªÙ…Ø§Ù„ Ø£ÙƒØ¨Ø± Ù…Ù† 0.5 ÙŠØ¹ØªØ¨Ø± ØªØµÙ†ÙŠÙ Ù…ÙˆØ¬ÙˆØ¯
6. ØªØ±Ø¬Ø¹ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©

---

## 1ï¸âƒ£4ï¸âƒ£ Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ

```python
input_abstract = """The dominant sequence transduction models are based on complex
recurrent or convolutional neural networks in an encoder-decoder configuration.
The best performing models also connect the encoder and decoder through an attention mechanism.
We propose a new simple network architecture, the Transformer, based solely on attention mechanisms,
dispensing with recurrence and convolutions entirely..."""

predicted_terms = predict_subject_areas(input_abstract, loaded_model, loaded_vectorizer, loaded_mlb)
print("Predicted subject areas:", predicted_terms)
```

**Ù‡Ø°Ø§ Ø§Ù„Ù…Ø«Ø§Ù„:**
- Ù…Ù„Ø®Øµ ÙˆØ±Ù‚Ø© Ø§Ù„Ù€ Transformer Ø§Ù„Ø´Ù‡ÙŠØ±Ø©
- Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù‡ÙŠØªÙˆÙ‚Ø¹ Ø¥Ù†Ù‡Ø§ ÙÙŠ Ù…Ø¬Ø§Ù„Ø§Øª Ø²ÙŠ: Machine Learning, Natural Language Processing, Ø¥Ù„Ø®

---

## Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ğŸ¯

### Ù…Ø§ Ø¹Ù…Ù„Ù‡ Ø§Ù„ÙƒÙˆØ¯:
1. âœ… Ù‚Ø±Ø£ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ÙˆØ±Ø§Ù‚ Ø§Ù„Ø¨Ø­Ø«ÙŠØ©
2. âœ… Ù†Ø¸Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ£Ø²Ø§Ù„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
3. âœ… Ù‚Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØªØ­Ù‚Ù‚ ÙˆØ§Ø®ØªØ¨Ø§Ø±
4. âœ… Ø­ÙˆÙ„ Ø§Ù„Ù†ØµÙˆØµ Ù„Ø£Ø±Ù‚Ø§Ù… Ø¨Ù€ TF-IDF
5. âœ… Ø­ÙˆÙ„ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ù„Ù€ binary encoding
6. âœ… Ø¨Ù†Ù‰ Ù†Ù…ÙˆØ°Ø¬ deep learning Ø¨Ù€ 4 Ø·Ø¨Ù‚Ø§Øª Ù…Ø®ÙÙŠØ©
7. âœ… Ø¯Ø±Ø¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ù…Ù†Ø¹ overfitting
8. âœ… Ù‚ÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ±Ø³Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬
9. âœ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„Ø§Ø­Ù‚Ø§Ù‹
10. âœ… Ø§Ø®ØªØ¨Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„ Ø­Ù‚ÙŠÙ‚ÙŠ

### Ø§Ù„Ù†ØªÙŠØ¬Ø©:
Ù†Ù…ÙˆØ°Ø¬ Ø°ÙƒÙŠ ÙŠÙ‚Ø¯Ø± ÙŠØ´ÙˆÙ Ù…Ù„Ø®Øµ Ø£ÙŠ ÙˆØ±Ù‚Ø© Ø¨Ø­Ø«ÙŠØ© ÙˆÙŠÙ‚ÙˆÙ„ Ø¥ÙŠÙ‡ Ù…Ø¬Ø§Ù„Ù‡Ø§ Ø§Ù„Ø¹Ù„Ù…ÙŠ! ğŸš€
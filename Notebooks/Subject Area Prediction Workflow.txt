---
مشروع: تصنيف وتوصية الأوراق البحثية (Subject Area Prediction)

# Workflow تفصيلي لتقسيم العمل على 12 شخص

1. **قراءة البيانات من ملف CSV**
   - تحميل البيانات باستخدام pandas من ملف CSV.
   - التأكد من تحميل البيانات بشكل صحيح (عرض أول 5 صفوف).
   - التأكد من أسماء الأعمدة وصحة البيانات الأولية.

2. **استكشاف الأعمدة الأساسية**
   - عرض أسماء الأعمدة وأنواع البيانات (dtypes).
   - حساب عدد القيم الفريدة في كل عمود.
   - التأكد من عدم وجود أعمدة غير ضرورية.

3. **تحليل الأعمدة الفريدة وتوزيعها**
   - تحليل عمود terms (عدد التكرارات لكل تصنيف).
   - تحليل titles و abstracts (مثلاً: متوسط طول النص، وجود نصوص فارغة).
   - رسم إحصائيات بسيطة (اختياري).

4. **كشف التكرارات في البيانات**
   - حساب عدد الصفوف المكررة في الداتا.
   - تحديد أماكن التكرار (duplicated rows).

5. **تنظيف البيانات من التكرارات والقيم غير المرغوبة**
   - إزالة الصفوف المكررة.
   - معالجة القيم الفارغة أو غير الصحيحة في الأعمدة الأساسية.
   - التأكد من أن كل صف يحتوي على terms وabstract وtitle.

6. **تجهيز الأعمدة المستهدفة (Target Preparation)**
   - تحويل عمود terms من نص إلى قائمة (list) باستخدام literal_eval.
   - تجهيز MultiLabelBinarizer لتحويل التصنيفات إلى مصفوفة أرقام (one-hot encoding).
   - حفظ أسماء التصنيفات (classes) لاستخدامها لاحقًا.

7. **تقسيم البيانات إلى train/test/validation**
   - استخدام train_test_split لتقسيم البيانات بنسبة مناسبة (مثلاً: 80% train، 10% test، 10% validation).
   - استخدام stratify لضمان توزيع التصنيفات بشكل متوازن.
   - التأكد من حجم كل جزء بعد التقسيم.

8. **معالجة النصوص وتحويلها إلى خصائص رقمية (Text Vectorization)**
   - استخدام TfidfVectorizer أو CountVectorizer لتحويل النصوص (abstracts) إلى مصفوفات رقمية.
   - تحديد حجم المفردات (vocab size) والمعلمات المناسبة (ngram_range، min_df، ...).
   - حفظ الـ vectorizer لاستخدامه لاحقًا في التوقع.

9. **تحويل البيانات إلى Generators أو تنسيق مناسب لـ TensorFlow**
   - بناء كلاس SparseDataGenerator لتحويل البيانات إلى دفعات صغيرة (batches) أثناء التدريب.
   - التأكد من أن generator يتعامل مع sparse matrices بكفاءة.
   - اختبار generator على دفعة واحدة للتأكد من صحة الأبعاد.

10. **بناء النموذج العميق (Model Building)**
    - تصميم طبقات النموذج (Dense layers, Dropout, Output layer).
    - اختيار دالة التفعيل (activation) المناسبة (مثل ReLU, Sigmoid).
    - تحديد عدد الطبقات والوحدات بناءً على حجم البيانات.

11. **تدريب النموذج (Model Training)**
    - تنفيذ عملية التدريب باستخدام fit مع EarlyStopping.
    - مراقبة الأداء على validation set أثناء التدريب.
    - حفظ أفضل وزن للنموذج (best weights).

12. **تقييم النموذج، حفظه، وتجربة التوقعات**
    - تقييم النموذج على train/validation/test (حساب الدقة والخسارة).
    - حفظ النموذج النهائي وملفات الـ vectorizer و MultiLabelBinarizer.
    - كتابة دالة للتوقع (prediction) على نص جديد، وتجربة التوقع على مثال عملي.

---

# أصعب 3 أجزاء في المشروع:
1. **تحويل البيانات إلى Generators والتعامل مع Sparse Matrices**
   - يتطلب معرفة جيدة بكيفية التعامل مع البيانات الضخمة وتحويلها إلى دفعات صغيرة بكفاءة.
2. **بناء وضبط النموذج العميق (Model Architecture & Tuning)**
   - يحتاج خبرة في تصميم الشبكات العصبية وضبط المعلمات لتحقيق أفضل أداء.
3. **تقييم النموذج وتجربة التوقعات (Evaluation & Inference)**
   - يتطلب فهم كيفية تقييم نموذج متعدد التصنيفات وتجهيز دوال التوقع بشكل عملي.

---

كل شخص مسؤول عن تنفيذ جزءه بشكل كامل مع توثيق الخطوات والنتائج، والتأكد من جاهزية الجزء الذي يعمل عليه للدمج مع باقي الأجزاء. 